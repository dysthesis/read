#!/usr/bin/env sh
set -eu

XDG_CACHE_HOME=${XDG_CACHE_HOME:-"$HOME/.local/share"}
XDG_CONFIG_HOME=${XDG_CONFIG_HOME:-"$HOME/.config"}
CACHE_DIR=${CACHE_DIR:-"$XDG_CACHE_HOME/read"}
CACHE_FILE="$CACHE_DIR/contents.json"

READ_FILE="$CACHE_DIR/read.txt"
SHOW_FLAG="$CACHE_DIR/show_read.flag" # its existence means "show all"

is_read() { grep -Fxq "$1" "$READ_FILE" 2>/dev/null; }
mark_read() { is_read "$1" || printf '%s\n' "$1" >>"$READ_FILE"; }
unmark_read() { [ -f "$READ_FILE" ] && grep -Fxv "$1" "$READ_FILE" >"$READ_FILE.$$" && mv "$READ_FILE.$$" "$READ_FILE"; }
toggle_show() { [ -e "$SHOW_FLAG" ] && rm -f "$SHOW_FLAG" || : >"$SHOW_FLAG"; }

export READ_FILE SHOW_FLAG

CLIP_CMD=${CLIP_CMD:-$(
  if command -v wl-copy >/dev/null 2>&1; then
    printf '%s' 'wl-copy'
  elif command -v xclip >/dev/null 2>&1; then
    printf '%s' 'xclip -selection clipboard'
  elif command -v pbcopy >/dev/null 2>&1; then
    printf '%s' 'pbcopy'
  else printf '%s' 'cat'; fi # fallback does nothing, but never crashes
)}
export CLIP_CMD

SHELL="sh"

LIKED_FILE="$CACHE_DIR/liked.txt"
DISLIKED_FILE="$CACHE_DIR/disliked.txt"

URLS=${URLS:-$XDG_CONFIG_HOME/read/feeds} # file containing feed URLs, one per line
JOBS=${JOBS:-64}                          # maximum concurrent feed workers

export CACHE_FILE LIKED_FILE DISLIKED_FILE

log() { printf '%s\n' "$*" >&2; }

GIT=$(command -v git || true)

init_git_repo() {
  [ -n "$GIT" ] || return
  if [ ! -d "$CACHE_DIR/.git" ]; then
    git -C "$CACHE_DIR" init -q # new repo
  fi
}

commit_if_changed() {
  [ -n "$GIT" ] || return
  # Take an exclusive lock to avoid concurrent writers
  exec 9>"$CACHE_DIR/.git/commit.lock" && flock -n 9 || return # silently skip if busy
  git -C "$CACHE_DIR" add -A
  if ! git -C "$CACHE_DIR" diff-index --quiet HEAD --; then # changes?
    git -C "$CACHE_DIR" commit -q -m "state: $(date -Iseconds)"
  fi
  flock -u 9 # release
}

init_git_repo

# emit $1 as a single-quoted shell string, POSIX portable
shell_quote() {
  printf "'%s'" "$(printf '%s' "$1" | sed "s/'/'\\\\''/g")"
}
update_feeds() {
  tmp=$(mktemp -d) || exit 1
  trap 'rm -rf "$tmp" "$sem"' EXIT INT HUP TERM

  sem=$(mktemp -u)
  mkfifo "$sem"
  exec 3<>"$sem" 4>&3
  rm "$sem" # remove pathname; FIFO stays open
  i=0
  while [ "$i" -lt "$JOBS" ]; do # preload N tokens
    printf '\n' >&4
    i=$((i + 1))
  done

  i=0
  while IFS= read -r feed || [ -n "$feed" ]; do
    IFS= read -r _ <&3 # wait for a free token
    i=$((i + 1))

    {
      log "[#${i}] Fetching $feed..."

      if ! data=$(curl -sL --retry 3 --max-time 20 "$feed"); then
        log "[#${i}] curl failed ($?) – feed ignored"
        printf '[]' >"$tmp/$i.json"
        printf '\n' >&4 # return token
        exit 0
      fi

      json=$(printf '%s' "$data" | r 2>"$tmp/r-${i}.err")
      r_status=$?
      if [ "$r_status" -ne 0 ]; then
        log "[#${i}] r exited $r_status – feed ignored"
        printf '[]' >"$tmp/$i.json"
        printf '\n' >&4
        exit 0
      fi

      printf '%s\n' "$json" |
        jq -c '.[]' | # stream objects, one per line
        while IFS= read -r art; do
          url=$(printf '%s\n' "$art" | jq -r '.url // empty')
          [ -z "$url" ] && {
            printf '%s\n' "$art"
            continue
          }

          tmp_body=$(mktemp) || {
            log "mktemp failed"
            continue
          }

          # 1.  Produce Markdown → file (with safety belt)
          if timeout 30 clean --markdown "$url" 2>>"$CACHE_DIR/clean.err" >"$tmp_body"; then
            # 2.  Merge without huge argv / without python
            printf '%s\n' "$art" |
              jq --rawfile body "$tmp_body" '.content = $body'
          else
            log "[#${i}] body skipped – clean failed for $url"
            printf '%s\n' "$art"
          fi

          rm -f -- "$tmp_body"
        done |
        jq -s '.' >"$tmp/$i.json" # re-assemble array

      printf '\n' >&4 # return token
    } &
  done <"$URLS"

  wait

  log '[+] Merging...'
  find "$tmp" -name '*.json' -print0 |
    xargs -0 cat | jq -s 'add' >"$CACHE_FILE"

  log "[+] Written merged array to $CACHE_FILE"
  commit_if_changed
}

# score_one idx text
score_one() {
  idx=$1
  text=$2

  like=0
  dis=0

  # Create a secure temporary file for the article text
  tmp=$(mktemp -t read_score.XXXXXX) || {
    printf >&2 'mktemp failed\n'
    return 1
  }
  printf '%s' "$text" >"$tmp"

  if [ -s "$LIKED_FILE" ]; then
    like=$(sim "$tmp" "$LIKED_FILE" | awk '{print $NF+0}')
  fi
  if [ -s "$DISLIKED_FILE" ]; then
    dis=$(sim "$tmp" "$DISLIKED_FILE" | awk '{print $NF+0}')
  fi

  rm -f -- "$tmp" # always clean up

  # like − dislike
  awk -v a="$like" -v b="$dis" 'BEGIN{printf "%.3f", a-b}'
}

build_menu() {
  idx=0

  colors=$(tput colors 2>/dev/null || printf %s 0)
  if [ "$colors" -ge 256 ]; then
    AUTHOR_CLR=$(tput setaf 244)
  elif [ "$colors" -ge 16 ]; then
    AUTHOR_CLR=$(tput setaf 8)
  else
    AUTHOR_CLR=$(tput dim)
  fi
  TITLE_CLR=$(tput bold)
  RESET=$(tput sgr0)
  read_vis=$([ -e "$SHOW_FLAG" ] && echo 1 || echo 0)

  jq -c '.[]' "$CACHE_FILE" |
    while IFS= read -r line; do
      id=$(printf '%s\n' "$line" | jq -r '.id')
      if [ "$read_vis" -eq 0 ] && is_read "$id"; then
        continue # hide read articles
      fi
      # Extract the bits we need from the current JSON object
      title=$(printf '%s\n' "$line" | jq -r '.title')
      url=$(printf '%s\n' "$line" | jq -r '.url')
      author=$(printf '%s\n' "$line" |
        jq -r '(.author | select(length>0)) // .source_title // empty')
      content=$(printf '%s\n' "$line" | jq -r '.content')

      score=$(score_one "$idx" "$content")
      printf '%s\t%s\t%s\t%s\t%s\n' \
        "$score" "$idx" "${TITLE_CLR}${title}${RESET}" \
        "${AUTHOR_CLR} – ${author}${RESET}" \
        "${AUTHOR_CLR}${score}${RESET}"

      idx=$((idx + 1))
    done | sort -r -n -k1,1 # highest score first
}

preview_cmd() {
  cat <<'EOS'
idx={2}
jq -r ".[$idx].content" "$CACHE_FILE" | mdcat
EOS
}

open_viewer() {
  idx=$1
  jq -r ".[$idx].content" "$CACHE_FILE" | mdcat -p --columns=100
}

open_and_mark() {
  idx=$1
  id=$(jq -r ".[$idx].id" "$CACHE_FILE")
  mark_read "$id"
  open_viewer "$idx"
}

append_feedback() {
  idx=$1 file=$2
  jq -r ".[$idx].content" "$CACHE_FILE" >>"$file"
  printf '\n' >>"$file"
  commit_if_changed
}

copy_url() { # copy_url <idx>
  idx=$1
  url=$(jq -r ".[$idx].url // empty" "$CACHE_FILE") || return
  [ -n "$url" ] || return # nothing to copy
  printf '%s' "$url" | eval "$CLIP_CMD" >/dev/null 2>&1
}

fzf_loop() {
  build_menu |
    fzf --ansi \
      --multi \
      --no-sort \
      --header="read - feed reader" \
      --header-first \
      --prompt="Search: " \
      --layout=reverse \
      --delimiter='\t' \
      --with-nth=3.. \
      --preview "$(preview_cmd)" \
      --preview-window=right:60%:wrap \
      --bind "alt-l:execute-silent($(shell_quote "$0") __like {2})+reload($(shell_quote "$0") __menu)" \
      --bind "alt-d:execute-silent($(shell_quote "$0") __dislike {2})+reload($(shell_quote "$0") __menu)" \
      --bind "alt-y:execute-silent($(shell_quote "$0") __copy {2})" \
      --bind "enter:execute($(shell_quote "$0") __view {2})+reload($(shell_quote "$0") __menu)" \
      --bind "alt-u:execute-silent($(shell_quote "$0") __unread {2}) \
                +reload($(shell_quote "$0") __menu)" \
      --bind "alt-r:execute-silent($(shell_quote "$0") __toggle)+reload($(shell_quote "$0") __menu)"
}

case ${1:-show} in
update) update_feeds ;;
__view)
  shift
  open_and_mark "$1"
  ;;
__like)
  shift
  append_feedback "$1" "$LIKED_FILE"
  ;;
__dislike)
  shift
  append_feedback "$1" "$DISLIKED_FILE"
  ;;
__menu) build_menu ;;
__markread)
  shift
  id=$(jq -r ".[$1].id" "$CACHE_FILE")
  mark_read "$id"
  commit_if_changed
  ;;
__unread)
  shift
  id=$(jq -r ".[$1].id" "$CACHE_FILE")
  unmark_read "$id"
  commit_if_changed
  ;;
__toggle) toggle_show ;; # no commit; purely a view flag
__copy)
  shift
  copy_url "$1"
  ;;
show | *) fzf_loop ;;
esac
